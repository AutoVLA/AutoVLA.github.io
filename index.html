<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning.">
  <meta name="keywords" content="VLA, End-to-end Autonomous Driving">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AutoVLA</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/AutoVLA-icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="logo-title-container">
            <img src="./static/images/AutoVLA-Logo.png" alt="AutoVLA Logo" class="publication-logo">
            <h1 class="title is-3 publication-title">AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning</h1>
          </div>
          <div class="is-size-5" style="margin-top: -0.5rem; margin-bottom: 0rem;">
            - NeurIPS 2025 -
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zewei-zhou.github.io/">Zewei Zhou</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://www.tianhui-vicky.com/">Tianhui Cai</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://sethzhao506.github.io/">Seth Z. Zhao</a>,
            </span>
            <span class="author-block">
              <a href="https://handsomeyun.github.io/">Yun Zhang</a>,
            </span>
            <span class="author-block">
              <a href="https://mczhi.github.io/">Zhiyu Huang</a><sup>‚Ä†</sup>,
            </span>
            <span class="author-block">
              <a href="https://boleizhou.github.io/">Bolei Zhou</a>,
            </span>
            <span class="author-block">
              <a href="https://mobility-lab.seas.ucla.edu/about/">Jiaqi Ma</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of California, Los Angeles</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution. <sup>‚Ä†</sup>Project leader.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.13757"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.13757"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ucla-mobility/AutoVLA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-layer-group"></i>
                  </span>
                  <span>Data (Coming Soon)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser-image" src="./static/images/AutoVLA_framework.png" alt="AutoVLA Framework Diagram" style="width: 100%; max-width: 100%; height: auto; max-height: 70vh; object-fit: contain; margin-bottom: 1.2rem;">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">AutoVLA</span> integrates CoT reasoning and physical action tokenization to directly generate planning trajectories through a unified autoregressive process, dynamically switching dual-thinking modes.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in Vision-Language-Action (VLA) models have shown promise for end-to-end autonomous driving by leveraging world knowledge and reasoning capabilities. However, current VLA models often struggle with physically infeasible action outputs, complex model structures, and unnecessarily long reasoning. 
          </p>
          <p>
            In this paper, we propose <span class="dnerf">AutoVLA</span>, a novel VLA framework that unifies reasoning and action generation within a single autoregressive generation model. <span class="dnerf">AutoVLA</span> performs semantic reasoning and trajectory planning directly from raw visual inputs and language instructions. We tokenize continuous trajectories into discrete, feasible actions, enabling direct integration into the language model. For training, we employ supervised fine-tuning to equip the model with dual thinking modes: fast thinking (trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning). To further enhance planning performance and efficiency, we introduce a reinforcement fine-tuning method based on Group Relative Policy Optimization (GRPO), reducing unnecessary reasoning in straightforward scenarios.
          </p>
          <p>
            Extensive experiments across real-world and simulated datasets and benchmarks, including nuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of <span class="dnerf">AutoVLA</span> in both open-loop and closed-loop settings. Qualitative results further showcase the adaptive reasoning and accurate planning capabilities of <span class="dnerf">AutoVLA</span> in diverse scenarios. We will release the code, model weights, and datasets to facilitate future research in the field.
          </p>
        </div>
      </div>
    </div>
    
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Model Structure and Training Strategy</h2>
        </div>
      </div>
    </div>
    <div class="container is-max-desktop">
      <img src="./static/images/AutoVLA_framework_training.png" alt="AutoVLA Framework and Training Strategy" style="width: 100%; max-width: 100%; height: auto; max-height: 70vh; object-fit: contain; margin-bottom: 1.2rem; margin-top: 2.5rem;">
    </div>
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p style="margin-top: 0.25rem;">
              ‚öôÔ∏è Two Main Components:
            </p>
            <ul style="list-style-type: disc; margin-left: 40px;margin-top: -0.5rem;">
              <li><strong> VLM Backbone</strong> is capable of processing visual and textual input and generating corresponding tokens (reasoning and action), employing a unified autoregressive Transformer decoder.</li>
              <li><strong> Physical Action Token Generation</strong> extends the language model decoder to output physical action tokens, which are designed to comply with physical constraints and can be reliably translated into physically feasible trajectories.</li>
            </ul>
            
            <p style="margin-top: 1.5rem;">
              ü™ú Two Training Stages:
            </p>
            <ul style="list-style-type: disc; margin-left: 40px;margin-top: -0.5rem;">
              <li><strong> Supervised Fine-Tuning (SFT)</strong> aims to jointly learn reasoning & action and enable dual thinking capabilities, using ground-truth trajectory data and distilling high-quality reasoning data from a large-scale VLM model.</li>
              <li><strong> Reinforcement Fine-Tuning (RFT)</strong> uses task-specific reward functions to optimize planning performance while enabling adaptive reasoning and improving its running efficiency by minimizing unnecessary reasoning.</li>
            </ul>
          </div>
        </div>
      </div>

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src=""
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Experiments</h2>

        <div class="columns">
          <div class="column is-half">
            <h3 class="title is-4" style="text-align: left;">Data Scaling</h3>
            <div class="content has-text-justified">
              <p>
                In this experiment, <span class="dnerf">AutoVLA</span> is trained on a mixture of the nuPlan and nuScenes datasets with varying training set sizes (10k, 50k, 100k, 185k). 
              </p>
              <ul style="list-style-type: disc; margin-left: 40px;margin-top: -0.5rem;">
                <li style="margin-top: 1.5rem;">Increasing the amount of training data consistently improves planning performance.</li>
                <li style="margin-top: 1rem;">Learning structured reasoning requires enough training data and can further improve planning performance.</li>
                <li style="margin-top: 1rem;">The action-only supervision yields better performance in simpler scenario datasets, such as the nuScenes dataset.</li>
              </ul>
            </div>
          </div>
          <div class="column is-half">
            <img src="./static/images/sft_all_metrics_log.png" alt="Data Scaling Metrics" style="width: 100%; height: auto;">
          </div>
        </div>
        <br/>

        <h3 class="title is-4" style="text-align: left;">RFT Performance</h3>
        <div class="content has-text-justified">
          <div class="container is-max-desktop">
            <img src="./static/images/rl_vis.png" alt="RFT Visualization" style="width: 100%; max-width: 100%; height: auto; max-height: 70vh; object-fit: contain; margin-bottom: 1.2rem;">
          </div>
          <p>
            We apply RFT to the full-data CoT reasoning model trained via SFT.
            <ul style="list-style-type: disc; margin-left: 40px;margin-top: -0.5rem;">
              <li>10.6% improvement in PDMS on the NAVSIM testing set and a 66.8% reduction in runtime (500 samples) with RFT.</li>
              <li>Larger groups lead to better performance by promoting a broader exploration of training samples.</li>
              <li>RFT reduces unnecessary and slow reasoning in simple scenarios.</li>
            </ul>
          </p>
        </div>

        <h3 class="title is-4" style="text-align: left; margin-top: 3rem;">nuPlan Results</h3>
        <div class="content has-text-justified">
          <div class="container is-max-desktop">
            <img src="./static/images/nuplan_vis.png" alt="nuPlan Visualization" style="width: 100%; max-width: 100%; height: auto; max-height: 100vh; object-fit: contain; margin-bottom: 1.2rem;">
          </div>
        </div>

        <h3 class="title is-4" style="text-align: left;">Waymo End-to-End Driving Results</h3>
        <div class="content has-text-justified">
          <p>
            In the <a href="https://waymo.com/open/challenges/2025/e2e-driving/" target="_blank">Waymo Vision-based End-to-End Driving Challenge</a> (as of May 22, 2025), <span class="dnerf">AutoVLA</span> ranks highly in both RFS Overall and ADE metrics and achieves the top score in the RFS Spotlight metric, which focuses on the most challenging scenarios.
          </p>
          <div class="container is-max-desktop">
            <img src="./static/images/waymo_vis1.png" alt="Waymo Visualization" style="width: 100%; max-width: 100%; height: auto; max-height: 100vh; object-fit: contain; margin-bottom: 2rem;">
          </div>
        </div>

        <h3 class="title is-4" style="text-align: left; margin-bottom: 2rem; margin-top:2rem;">nuScenes Results</h3>
        <div class="content has-text-justified">
          <p>
            <span style="color: red; font-weight: bold;">Red lines</span> represent the planning trajectories, and <span style="color: green; font-weight: bold;">Green lines</span> represent the ground-truth trajectories.
          </p>
          <div class="container is-max-desktop">
            <div class="columns is-centered is-multiline">
              <div class="column is-full has-text-centered" style="padding: 0.5rem;">
                <video class="nuscene-video" src="./static/images/nuscene_2.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
              </div>
              <div class="column is-full has-text-centered" style="padding: 0.5rem;">
                <video class="nuscene-video" src="./static/images/nuscene_3.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
              </div>
              <div class="column is-full has-text-centered" style="padding: 0.5rem;">
                <video class="nuscene-video" src="./static/images/nuscene_4.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
              </div>
              <div class="column is-full has-text-centered" style="padding: 0.5rem;">
                <video class="nuscene-video" src="./static/images/nuscenes_1.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
              </div>
            </div>
          </div>
        </div>

        <h3 class="title is-4" style="text-align: left; margin-bottom: 2rem; margin-top:4rem">CARLA Closed-loop Results</h3>
        <div class="content has-text-justified">
          <div class="container is-max-desktop">
            <div class="columns is-centered is-multiline">
              <div class="column is-half has-text-centered" style="padding: 0.5rem;">
                <video src="./static/images/carla_sudden_pedestrian_emergence.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
                <div>Parking Crossing Pedestrian</div>
              </div>
              <div class="column is-half has-text-centered" style="padding: 0.5rem;">
                <video src="./static/images/carla_yeliding_pedestrian.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
                <div>Vehicle Turning Route Pedestrian</div>
              </div>
              <div class="column is-half has-text-centered" style="padding: 0.5rem;">
                <video src="./static/images/carla_yeliding_cut_in.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
                <div>Parking Cut-In</div>
              </div>
              <div class="column is-half has-text-centered" style="padding: 0.5rem;">
                <video src="./static/images/carla_cut_in.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
                <div>Parked Obstacle</div>
              </div>
              <div class="column is-half has-text-centered" style="padding: 0.5rem;">
                <video src="./static/images/carla_lane_change_fog.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
                <div>Hazard At Side Lane</div>
              </div>
              <div class="column is-half has-text-centered" style="padding: 0.5rem;">
                <video src="./static/images/carla_stop_turn_left.mp4" controls autoplay loop muted playsinline style="width: 100%;"></video>
                <div>Opposite Vehicle Taking Priority</div>
              </div>
            </div>
          </div>
        </div>

      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhou2025autovla,
  author    = {Zhou, Zewei and Cai, Tianhui and Zhao, Seth Z.and Zhang, Yun and Huang, Zhiyu and Zhou, Bolei and Ma, Jiaqi},
  title     = {AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning},
  journal   = {arXiv preprint arXiv:2506.13757},
  year      = {2025},
}</code></pre>
  </div>
</section>



<div class="content has-text-centered">
  <p>
    The website design was adapted from <a href="https://nerfies.github.io/">nerfies</a>.
  </p>
</div>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    var videos = document.querySelectorAll('.nuscene-video');
    videos.forEach(function(video) {
      video.playbackRate = 1.2;
    });
  });
</script>

</body>
</html>
